---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.7.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---


# Assignment 2 - Introduction to NLTK

In part 1 of this assignment you will use NLTK to explore the Herman Melville
novel Moby Dick. Then in part 2 you will create a spelling recommender function
that uses NLTK to find words similar to the misspelling.


## Part 1 - Analyzing Moby Dick


```{python}
import nltk
import numpy as np
import pandas as pd


# If you would like to work with the raw text, you can use 'moby_raw'.
with open('moby.txt', 'r') as f:
    moby_raw = f.read()

# If you would like to work with the novel nltk.Text format, you can use 'text1'.
moby_tokens = nltk.word_tokenize(moby_raw)
text1 = nltk.Text(moby_tokens)
```


### Example 1

How many tokens (words and punctuation symbols) are in `text1`?

*This function should return an integer.*


```{python}
def example_one():

    # Solution 1:
    sol_1 = len(nltk.word_tokenize(moby_raw))

    # Solution 2:
    sol_2 = len(text1)

    # Testing:
    assert sol_1 == sol_2
    assert type(sol_1) == int

    return sol_1


example_one()
```


### Example 2

How many unique tokens (unique words and punctuation) does `text1` have?

*This function should return an integer.*


```{python}
def example_two():

    # Solution 1:
    sol_1 = len(set(nltk.word_tokenize(moby_raw)))

    # Solution 2:
    sol_2 = len(set(text1))

    # Testing:
    assert sol_1 == sol_2
    assert type(sol_1) == int

    return sol_1


example_two()
```


### Example 3

After lemmatizing the verbs, how many unique tokens does `text1` have?

*This function should return an integer.*


```{python}
from nltk.stem import WordNetLemmatizer


def example_three():

    lemmatizer = WordNetLemmatizer()
    lemmatized = [lemmatizer.lemmatize(word, 'v') for word in text1]
    sol = len(set(lemmatized))

    # Testing:
    assert type(sol) == int

    return sol


example_three()
```


### Question 1

What is the lexical diversity (ratio of unique tokens to the total number of
tokens) of the given text input?

*This function should return a float.*


```{python}
def answer_one():

    # Number of unique tokens:
    num_uniq_toks = example_two()

    # Total number of tokens:
    num_toks = example_one()

    # Lexical diversity:
    lex_div = num_uniq_toks / num_toks

    # Testing:
    assert type(lex_div) == float

    return lex_div


answer_one()
```


### Question 2

What percentage of tokens is 'whale' or 'Whale'?

*This function should return a float.*


```{python}
def answer_two():

    # Number of occurrences of the tokens 'whale' and 'Whale':
    num_whales = text1.count('whale') + text1.count('Whale')

    # Total number of tokens:
    num_toks = example_one()

    # Compute the percentage:
    percent = 100 * (num_whales / num_toks)

    # Testing:
    assert type(percent) == float

    return percent


answer_two()
```


### Question 3

What are the 20 most frequently occurring (unique) tokens in the text? What is
their frequency?

*This function should return a list of 20 tuples where each tuple is of the
form `(token, frequency)`. The list should be sorted in descending order of
frequency.*


```{python}
from nltk import FreqDist


def answer_three():

    # Compute the frequency of every unique token:
    freqs = FreqDist(text1)

    # Get the 20 most common unique tokens:
    top_20 = freqs.most_common(20)

    return top_20


answer_three()
```


### Question 4

What tokens have a length of greater than 5 and frequency of more than 150?

*This function should return an alphabetically sorted list of the tokens that
match the above constraints. To sort your list, use `sorted()`.*


```{python}
def answer_four():

    # Compute the frequency of every unique token with a length greater than 5:
    freqs = FreqDist([token for token in moby_tokens if len(token) > 5])

    # Get every unique token that satisfies the constraints:
    token_list = [token for token in freqs.keys() if freqs[token] > 150]

    # Sort the list of tokens:
    token_list = sorted(token_list)

    return token_list


answer_four()
```


### Question 5

Find the longest word in `text1` and that word's length.

*This function should return a tuple `(longest_word, length)`.*


```{python}
def answer_five():

    # Get every unique word:
    word_list = np.array([token for token in set(moby_tokens) if token.isalpha()])

    # Compute the length of every word in word_list:
    lens = np.vectorize(len)(word_list)

    # Find the longest word and its length:
    idx_max = np.argmax(lens)
    longest_word = word_list[idx_max]
    length = lens[idx_max]

    return (longest_word, length)


answer_five()
```


### Question 6

What unique words have a frequency of more than 2000? What is their frequency?

Hint: You may want to use `isalpha()` to check if the token is a word and not
punctuation.

*This function should return a list of tuples of the form `(frequency, word)`
sorted in descending order of frequency.*


```{python}
def answer_six():

    return


answer_six()
```


### Question 7

What is the average number of tokens per sentence?

This function should return a float.

```{python}
def answer_seven():


    return # Your answer here

answer_seven()
```

### Question 8

What are the 5 most frequent parts of speech in this text? What is their frequency?

This function should return a list of tuples of the form (part_of_speech, frequency) sorted in descending order of frequency.

```{python}
def answer_eight():


    return # Your answer here

answer_eight()
```

## Part 2 - Spelling Recommender

For this part of the assignment you will create three different spelling
recommenders, that each take a list of misspelled words and recommends
a correctly spelled word for every word in the list.

For every misspelled word, the recommender should find find the word in
correct_spellings that has the shortest distance, and starts with the same
letter as the misspelled word, and return that word as a recommendation.

Each of the three different recommenders will use a different distance measure (outlined below).

Each of the recommenders should provide recommendations for the three default
words provided: ['cormulent', 'incendenece', 'validrate'].


```{python}
from nltk.corpus import words

correct_spellings = words.words()
```


### Question 9

For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:

[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words.

This function should return a list of length three:
['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation'].

```{python}
def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):


    return # Your answer here

answer_nine()
```

### Question 10

For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:

[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams of the two words.

This function should return a list of length three:
['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation'].

```{python}
def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):


    return # Your answer here

answer_ten()
```

### Question 11

For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:

[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)

This function should return a list of length three:
['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation'].

```{python}
def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):


    return # Your answer here

answer_eleven()
```
