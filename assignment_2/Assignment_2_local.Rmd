---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.7.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---


# Assignment 2 - Introduction to NLTK

In part 1 of this assignment you will use NLTK to explore the Herman Melville
novel Moby Dick. Then in part 2 you will create a spelling recommender function
that uses NLTK to find words similar to the misspelling.


## Part 1 - Analyzing Moby Dick


```{python}
import nltk
import numpy as np
from nltk import FreqDist
from nltk.corpus import words
from nltk.metrics.distance import edit_distance, jaccard_distance
from nltk.stem import WordNetLemmatizer
from nltk.util import ngrams


# If you would like to work with the raw text, you can use 'moby_raw'.
with open('moby.txt', 'r') as f:
    moby_raw = f.read()

# If you would like to work with the novel nltk.Text format, you can use 'text1'.
moby_tokens = nltk.word_tokenize(moby_raw)
text1 = nltk.Text(moby_tokens)
```


### Example 1

How many tokens (words and punctuation symbols) are in `text1`?

*This function should return an integer.*


```{python}
def example_one():

    # Solution 1:
    sol_1 = len(moby_tokens)

    # Solution 2:
    sol_2 = len(text1)

    # Testing:
    assert sol_1 == sol_2
    assert type(sol_1) == int

    return sol_1


example_one()
```


### Example 2

How many unique tokens (unique words and punctuation) does `text1` have?

*This function should return an integer.*


```{python}
def example_two():

    # Solution 1:
    sol_1 = len(set(moby_tokens))

    # Solution 2:
    sol_2 = len(set(text1))

    # Testing:
    assert sol_1 == sol_2
    assert type(sol_1) == int

    return sol_1


example_two()
```


### Example 3

After lemmatizing the verbs, how many unique tokens does `text1` have?

*This function should return an integer.*


```{python}
def example_three():

    lemmatizer = WordNetLemmatizer()
    lemmatized = [lemmatizer.lemmatize(word, 'v') for word in text1]
    sol = len(set(lemmatized))

    # Testing:
    assert type(sol) == int

    return sol


example_three()
```


### Question 1

What is the lexical diversity (ratio of unique tokens to the total number of
tokens) of the given text input?

*This function should return a float.*


```{python}
def answer_one():

    # Number of unique tokens:
    num_uniq_toks = example_two()

    # Total number of tokens:
    num_toks = example_one()

    # Lexical diversity:
    lex_div = num_uniq_toks / num_toks

    # Testing:
    assert type(lex_div) == float

    return lex_div


answer_one()
```


### Question 2

What percentage of tokens is 'whale' or 'Whale'?

*This function should return a float.*


```{python}
def answer_two():

    # Number of occurrences of the tokens 'whale' and 'Whale':
    num_whales = text1.count('whale') + text1.count('Whale')

    # Total number of tokens:
    num_toks = example_one()

    # Compute the percentage:
    percent = 100 * (num_whales / num_toks)

    # Testing:
    assert type(percent) == float

    return percent


answer_two()
```


### Question 3

What are the 20 most frequently occurring (unique) tokens in the text? What is
their frequency?

*This function should return a list of 20 tuples where each tuple is of the
form `(token, frequency)`. The list should be sorted in descending order of
frequency.*


```{python}
def answer_three():

    # Compute the frequency of every unique token:
    fdist = FreqDist(text1)

    # Get the 20 most common unique tokens:
    top_20 = fdist.most_common(20)

    # Testing:
    assert type(top_20) == list
    assert type(top_20[0]) == tuple
    assert type(top_20[0][0]) == str
    assert type(top_20[0][1]) == int

    return top_20


answer_three()
```


### Question 4

What tokens have a length of greater than 5 and frequency of more than 150?

*This function should return an alphabetically sorted list of the tokens that
match the above constraints. To sort your list, use `sorted()`.*


```{python}
def answer_four():

    # Compute the frequency of every unique token with a length greater than 5:
    fdist = FreqDist([token for token in moby_tokens if len(token) > 5])

    # Get every unique token that satisfies the constraints.
    # Sort the list of tokens.
    tokens = sorted([token for token in fdist.keys() if fdist[token] > 150])

    # Testing:
    assert type(tokens) == list
    assert type(tokens[0]) == str

    return tokens


answer_four()
```


### Question 5

Find the longest word in `text1` and that word's length.

*This function should return a tuple `(longest_word, length)`.*


```{python}
def answer_five():

    # Get every unique word:
    words = np.array([token for token in set(moby_tokens)])

    # Compute the length of every unique word:
    lens = np.vectorize(len)(words)

    # Find the longest word and its length:
    idx_max = np.argmax(lens)
    longest_word = (words[idx_max], lens[idx_max])

    # Testing:
    assert type(longest_word) == tuple
    assert type(longest_word[0]) == np.str_
    assert type(longest_word[1]) == np.int_

    return longest_word


answer_five()
```


### Question 6

What unique words have a frequency of more than 2000? What is their frequency?

Hint: You may want to use `isalpha()` to check if the token is a word and not
punctuation.

*This function should return a list of tuples of the form `(frequency, word)`
sorted in descending order of frequency.*


```{python}
def answer_six():

    # Compute the frequency of every unique word:
    fdist = FreqDist([token for token in moby_tokens if token.isalpha()])

    # Get every unique word with a frequency greater than 2000.
    # Create two lists. One of them stores the words that satisfy the constraint,
    # and the other contains the corresponding frequencies.
    freqs = []
    words = []
    for word, freq in fdist.items():
        if freq > 2000:
            freqs.append(freq)
            words.append(word)

    # Create the desired list of tuples:
    freqs_words = [(freqs[idx], words[idx]) for idx in np.argsort(np.array(freqs))[::-1]]

    # Alternative (shorter) solution:
    freqs_words_alt = [(tpl[1], tpl[0]) for tpl in fdist.most_common(len(fdist)) if tpl[1] > 2000]

    # Testing:
    assert freqs_words == freqs_words_alt
    assert type(freqs_words) == list
    assert type(freqs_words[0]) == tuple
    assert type(freqs_words[0][0]) == int
    assert type(freqs_words[0][1]) == str

    return freqs_words


answer_six()
```


### Question 7

What is the average number of tokens per sentence?

*This function should return a float.*


```{python}
def answer_seven():

    # Get every sentence in the text:
    moby_sents = np.array(nltk.sent_tokenize(moby_raw))

    # Compute the number of tokens for every sentence:
    num_toks = np.vectorize(lambda sent: len(nltk.word_tokenize(sent)))(moby_sents)

    # Compute the average number of tokens per sentence:
    avg_num_toks = num_toks.mean()

    # Testing:
    assert type(avg_num_toks) == np.float_

    return avg_num_toks


answer_seven()
```


### Question 8

What are the 5 most frequent parts of speech in this text? What is their frequency?

*This function should return a list of tuples of the form `(part_of_speech, frequency)`
sorted in descending order of frequency.*


```{python}
def answer_eight():

    # Compute the frequency of every POS tag:
    fdist = FreqDist([tpl[1] for tpl in nltk.pos_tag(moby_tokens)])

    # Get the 5 most frequent POS tags:
    top_5 = fdist.most_common(5)

    # Testing:
    assert type(top_5) == list
    assert type(top_5[0]) == tuple
    assert type(top_5[0][0]) == str
    assert type(top_5[0][1]) == int

    return top_5


answer_eight()
```


## Part 2 - Spelling Recommender

For this part of the assignment you will create three different spelling
recommenders, that each take a list of misspelled words and recommends
a correctly spelled word for every word in the list.

For every misspelled word, the recommender should find the word in
`correct_spellings` that has the shortest distance, and starts with the same
letter as the misspelled word, and return that word as a recommendation.

Each of the three different recommenders will use a different distance measure
(outlined below).

Each of the recommenders should provide recommendations for the three default
words provided: `['cormulent', 'incendenece', 'validrate']`.


```{python}
# List containing the correctly spelled words:
correct_spellings = words.words()
```


### Question 9

For this recommender, your function should provide recommendations for the
three default words provided above using the following distance metric:

[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams
of the two words.

*This function should return a list of length three:
`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*


```{python}
def my_jaccard_distance(set_1, set_2):
    """Compute the Jaccard distance between two sets.

    Arguments:
        set_1: 1st set.
        set_2: 2nd set.

    Returns:
        jd: Jaccard distance between set_1 and set_2.
    """

    # Compute the cardinality of the intersection of the two sets:
    card_inter = len(set_1.intersection(set_2))

    # Compute the cardinality of the union of the two sets:
    card_union = len(set_1.union(set_2))

    # Compute the Jaccard distance between the two sets:
    jd = 1 - card_inter / card_union

    return jd


# A few tests:
assert my_jaccard_distance({1, 2, 3, 4}, {1, 2, 3, 4}) == 0
assert my_jaccard_distance({1, 2, 3, 4}, {3, 4}) == 0.5
assert my_jaccard_distance({1, 2, 3, 4}, {5, 6}) == 1


def recommend_word_jd(misspelled, n):
    """Recommend a correction for a single misspelled word. This function uses
       the Jaccard distance as the distance metric.

    Arguments:
        misspelled: String corresponding to the misspelled word.
        n: Size of the n-grams involved in the calculations of the Jaccard distance.

    Returns:
        recommend_1: Recommended correction for the misspelled word.
    """

    # Get the set of n-grams corresponding to the misspelled word:
    ngrams_misspelled = set(ngrams(misspelled, n))

    # Get the correctly spelled words that have the same first letter as the misspelled word:
    possbl_recommends = [word for word in correct_spellings if word[0] == misspelled[0]]

    # For every word selected above, compute the Jaccard distance between the
    # corresponding set of n-grams and ngrams_misspelled:
    jds_1 = [jaccard_distance(set(ngrams(word, n)), ngrams_misspelled) for word in possbl_recommends]
    jds_2 = [my_jaccard_distance(set(ngrams(word, n)), ngrams_misspelled) for word in possbl_recommends]

    # Find the correctly spelled word associated with the shortest Jaccard distance:
    recommend_1 = possbl_recommends[np.array(jds_1).argmin()]
    recommend_2 = possbl_recommends[np.array(jds_2).argmin()]

    # Testing:
    assert recommend_1 == recommend_2
    assert type(recommend_1) == str

    return recommend_1


def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):

    # Recommend a correction for every misspelled word in 'entries'.
    # This function uses the Jaccard distance between trigrams.
    recommends = [recommend_word_jd(misspelled=entry, n=3) for entry in entries]

    return recommends


answer_nine()
```


### Question 10

For this recommender, your function should provide recommendations for the
three default words provided above using the following distance metric:

[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams
of the two words.

*This function should return a list of length three:
`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*


```{python}
def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):

    # Recommend a correction for every misspelled word in 'entries'.
    # This function uses the Jaccard distance between 4-grams.
    recommends = [recommend_word_jd(misspelled=entry, n=4) for entry in entries]

    return recommends


answer_ten()
```


### Question 11

For this recommender, your function should provide recommendations for the
three default words provided above using the following distance metric:

[Edit distance on the two words with
transpositions](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance).

*This function should return a list of length three:
`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*


```{python}
def recommend_word_ed(misspelled):
    """Recommend a correction for a single misspelled word. This function uses
       the edit distance as the distance metric.

    Arguments:
        misspelled: String corresponding to the misspelled word.

    Returns:
        recommend: Recommended correction for the misspelled word.
    """

    # Get the correctly spelled words that have the same first letter as the misspelled word:
    possbl_recommends = [word for word in correct_spellings if word[0] == misspelled[0]]

    # For every word selected above, compute the edit distance between the word and 'misspelled':
    eds = [edit_distance(word, misspelled) for word in possbl_recommends]

    # Find the correctly spelled word associated with the shortest edit distance:
    recommend = possbl_recommends[np.array(eds).argmin()]

    # Testing:
    assert type(recommend) == str

    return recommend


def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):

    # Recommend a correction for every misspelled word in 'entries'.
    # This function uses the edit distance between words.
    recommends = [recommend_word_ed(misspelled=entry) for entry in entries]

    return recommends


answer_eleven()
```
