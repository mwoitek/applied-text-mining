{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 2 - Introduction to NLTK\n",
    "\n",
    "In part 1 of this assignment you will use NLTK to explore the Herman Melville\n",
    "novel Moby Dick. Then in part 2 you will create a spelling recommender function\n",
    "that uses NLTK to find words similar to the misspelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Analyzing Moby Dick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# If you would like to work with the raw text, you can use 'moby_raw'.\n",
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()\n",
    "\n",
    "# If you would like to work with the novel nltk.Text format, you can use 'text1'.\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)\n",
    "text1 = nltk.Text(moby_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "How many tokens (words and punctuation symbols) are in `text1`?\n",
    "\n",
    "*This function should return an integer.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255018"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_one():\n",
    "\n",
    "    # Solution 1:\n",
    "    sol_1 = len(moby_tokens)\n",
    "\n",
    "    # Solution 2:\n",
    "    sol_2 = len(text1)\n",
    "\n",
    "    # Testing:\n",
    "    assert sol_1 == sol_2\n",
    "    assert type(sol_1) == int\n",
    "\n",
    "    return sol_1\n",
    "\n",
    "\n",
    "example_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "How many unique tokens (unique words and punctuation) does `text1` have?\n",
    "\n",
    "*This function should return an integer.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20754"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_two():\n",
    "\n",
    "    # Solution 1:\n",
    "    sol_1 = len(set(moby_tokens))\n",
    "\n",
    "    # Solution 2:\n",
    "    sol_2 = len(set(text1))\n",
    "\n",
    "    # Testing:\n",
    "    assert sol_1 == sol_2\n",
    "    assert type(sol_1) == int\n",
    "\n",
    "    return sol_1\n",
    "\n",
    "\n",
    "example_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "After lemmatizing the verbs, how many unique tokens does `text1` have?\n",
    "\n",
    "*This function should return an integer.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16899"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_three():\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word, 'v') for word in text1]\n",
    "    sol = len(set(lemmatized))\n",
    "\n",
    "    # Testing:\n",
    "    assert type(sol) == int\n",
    "\n",
    "    return sol\n",
    "\n",
    "\n",
    "example_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What is the lexical diversity (ratio of unique tokens to the total number of\n",
    "tokens) of the given text input?\n",
    "\n",
    "*This function should return a float.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08138249064771899"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_one():\n",
    "\n",
    "    # Number of unique tokens:\n",
    "    num_uniq_toks = example_two()\n",
    "\n",
    "    # Total number of tokens:\n",
    "    num_toks = example_one()\n",
    "\n",
    "    # Lexical diversity:\n",
    "    lex_div = num_uniq_toks / num_toks\n",
    "\n",
    "    # Testing:\n",
    "    assert type(lex_div) == float\n",
    "\n",
    "    return lex_div\n",
    "\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What percentage of tokens is 'whale' or 'Whale'?\n",
    "\n",
    "*This function should return a float.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4125199005560392"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "\n",
    "    # Number of occurrences of the tokens 'whale' and 'Whale':\n",
    "    num_whales = text1.count('whale') + text1.count('Whale')\n",
    "\n",
    "    # Total number of tokens:\n",
    "    num_toks = example_one()\n",
    "\n",
    "    # Compute the percentage:\n",
    "    percent = 100 * (num_whales / num_toks)\n",
    "\n",
    "    # Testing:\n",
    "    assert type(percent) == float\n",
    "\n",
    "    return percent\n",
    "\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What are the 20 most frequently occurring (unique) tokens in the text? What is\n",
    "their frequency?\n",
    "\n",
    "*This function should return a list of 20 tuples where each tuple is of the\n",
    "form `(token, frequency)`. The list should be sorted in descending order of\n",
    "frequency.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7308),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978),\n",
       " ('his', 2459),\n",
       " ('it', 2196),\n",
       " ('I', 2111),\n",
       " ('!', 1767),\n",
       " ('is', 1722),\n",
       " ('--', 1713),\n",
       " ('with', 1659),\n",
       " ('he', 1658),\n",
       " ('was', 1639),\n",
       " ('as', 1620)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three():\n",
    "\n",
    "    # Compute the frequency of every unique token:\n",
    "    fdist = FreqDist(text1)\n",
    "\n",
    "    # Get the 20 most common unique tokens:\n",
    "    top_20 = fdist.most_common(20)\n",
    "\n",
    "    # Testing:\n",
    "    assert type(top_20) == list\n",
    "    assert type(top_20[0]) == tuple\n",
    "    assert type(top_20[0][0]) == str\n",
    "    assert type(top_20[0][1]) == int\n",
    "\n",
    "    return top_20\n",
    "\n",
    "\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What tokens have a length of greater than 5 and frequency of more than 150?\n",
    "\n",
    "*This function should return an alphabetically sorted list of the tokens that\n",
    "match the above constraints. To sort your list, use `sorted()`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain',\n",
       " 'Pequod',\n",
       " 'Queequeg',\n",
       " 'Starbuck',\n",
       " 'almost',\n",
       " 'before',\n",
       " 'himself',\n",
       " 'little',\n",
       " 'seemed',\n",
       " 'should',\n",
       " 'though',\n",
       " 'through',\n",
       " 'whales',\n",
       " 'without']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_four():\n",
    "\n",
    "    # Compute the frequency of every unique token with a length greater than 5:\n",
    "    fdist = FreqDist([token for token in moby_tokens if len(token) > 5])\n",
    "\n",
    "    # Get every unique token that satisfies the constraints.\n",
    "    # Sort the list of tokens.\n",
    "    tokens = sorted([token for token in fdist.keys() if fdist[token] > 150])\n",
    "\n",
    "    # Testing:\n",
    "    assert type(tokens) == list\n",
    "    assert type(tokens[0]) == str\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Find the longest word in `text1` and that word's length.\n",
    "\n",
    "*This function should return a tuple `(longest_word, length)`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('uninterpenetratingly', 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_five():\n",
    "\n",
    "    # Get every unique word:\n",
    "    words = np.array([token for token in set(moby_tokens) if token.isalpha()])\n",
    "\n",
    "    # Compute the length of every unique word:\n",
    "    lens = np.vectorize(len)(words)\n",
    "\n",
    "    # Find the longest word and its length:\n",
    "    idx_max = np.argmax(lens)\n",
    "    longest_word = (words[idx_max], lens[idx_max])\n",
    "\n",
    "    # Testing:\n",
    "    assert type(longest_word) == tuple\n",
    "    assert type(longest_word[0]) == np.str_\n",
    "    assert type(longest_word[1]) == np.int_\n",
    "\n",
    "    return longest_word\n",
    "\n",
    "\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What unique words have a frequency of more than 2000? What is their frequency?\n",
    "\n",
    "Hint: You may want to use `isalpha()` to check if the token is a word and not\n",
    "punctuation.\n",
    "\n",
    "*This function should return a list of tuples of the form `(frequency, word)`\n",
    "sorted in descending order of frequency.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13715, 'the'),\n",
       " (6513, 'of'),\n",
       " (6010, 'and'),\n",
       " (4545, 'a'),\n",
       " (4515, 'to'),\n",
       " (3908, 'in'),\n",
       " (2978, 'that'),\n",
       " (2459, 'his'),\n",
       " (2196, 'it'),\n",
       " (2111, 'I')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():\n",
    "\n",
    "    # Compute the frequency of every unique word:\n",
    "    fdist = FreqDist([token for token in moby_tokens if token.isalpha()])\n",
    "\n",
    "    # Get every unique word with a frequency greater than 2000.\n",
    "    # Create two lists. One of them stores the words that satisfy the constraint,\n",
    "    # and the other contains the corresponding frequencies.\n",
    "    freqs = []\n",
    "    words = []\n",
    "    for word, freq in fdist.items():\n",
    "        if freq > 2000:\n",
    "            freqs.append(freq)\n",
    "            words.append(word)\n",
    "\n",
    "    # Create the desired list of tuples:\n",
    "    freqs_words = [(freqs[idx], words[idx]) for idx in np.argsort(np.array(freqs))[::-1]]\n",
    "\n",
    "    # Alternative (shorter) solution:\n",
    "    freqs_words_alt = [(tpl[1], tpl[0]) for tpl in fdist.most_common(len(fdist)) if tpl[1] > 2000]\n",
    "\n",
    "    # Testing:\n",
    "    assert freqs_words == freqs_words_alt\n",
    "    assert type(freqs_words) == list\n",
    "    assert type(freqs_words[0]) == tuple\n",
    "    assert type(freqs_words[0][0]) == int\n",
    "    assert type(freqs_words[0][1]) == str\n",
    "\n",
    "    return freqs_words\n",
    "\n",
    "\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What is the average number of tokens per sentence?\n",
    "\n",
    "*This function should return a float.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.88489646772229"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_seven():\n",
    "\n",
    "    # Get every sentence in the text:\n",
    "    moby_sents = np.array(nltk.sent_tokenize(moby_raw))\n",
    "\n",
    "    # Compute the number of tokens for every sentence:\n",
    "    num_toks = np.vectorize(lambda sent: len(nltk.word_tokenize(sent)))(moby_sents)\n",
    "\n",
    "    # Compute the average number of tokens per sentence:\n",
    "    avg_num_toks = num_toks.mean()\n",
    "\n",
    "    # Testing:\n",
    "    assert type(avg_num_toks) == np.float_\n",
    "\n",
    "    return avg_num_toks\n",
    "\n",
    "\n",
    "answer_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "What are the 5 most frequent parts of speech in this text? What is their frequency?\n",
    "\n",
    "*This function should return a list of tuples of the form `(part_of_speech, frequency)`\n",
    "sorted in descending order of frequency.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 32730), ('IN', 28658), ('DT', 25870), (',', 19204), ('JJ', 17619)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_eight():\n",
    "\n",
    "    # Compute the frequency of every POS tag:\n",
    "    fdist = FreqDist([tpl[1] for tpl in nltk.pos_tag(moby_tokens)])\n",
    "\n",
    "    # Get the 5 most frequent POS tags:\n",
    "    top_5 = fdist.most_common(5)\n",
    "\n",
    "    # Testing:\n",
    "    assert type(top_5) == list\n",
    "    assert type(top_5[0]) == tuple\n",
    "    assert type(top_5[0][0]) == str\n",
    "    assert type(top_5[0][1]) == int\n",
    "\n",
    "    return top_5\n",
    "\n",
    "\n",
    "answer_eight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Spelling Recommender\n",
    "\n",
    "For this part of the assignment you will create three different spelling\n",
    "recommenders, that each take a list of misspelled words and recommends\n",
    "a correctly spelled word for every word in the list.\n",
    "\n",
    "For every misspelled word, the recommender should find find the word in\n",
    "correct_spellings that has the shortest distance, and starts with the same\n",
    "letter as the misspelled word, and return that word as a recommendation.\n",
    "\n",
    "Each of the three different recommenders will use a different distance measure (outlined below).\n",
    "\n",
    "Each of the recommenders should provide recommendations for the three default\n",
    "words provided: ['cormulent', 'incendenece', 'validrate'].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "\n",
    "correct_spellings = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words.\n",
    "\n",
    "This function should return a list of length three:\n",
    "['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "\n",
    "\n",
    "    return # Your answer here\n",
    "\n",
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams of the two words.\n",
    "\n",
    "This function should return a list of length three:\n",
    "['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "\n",
    "\n",
    "    return # Your answer here\n",
    "\n",
    "answer_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)\n",
    "\n",
    "This function should return a list of length three:\n",
    "['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "\n",
    "\n",
    "    return # Your answer here\n",
    "\n",
    "answer_eleven()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "r35En",
   "launcher_item_id": "tCVfW",
   "part_id": "NTVgL"
  },
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
