---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.8.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Assignment 4 - Document Similarity & Topic Modelling

## Part 1 - Document Similarity

For the first part of this assignment, you will complete the functions
`doc_to_synsets` and `similarity_score` which will be used by
`document_path_similarity` to find the path similarity between two documents.

The following functions are provided:
* **`convert_tag`**: converts the tag given by `nltk.pos_tag` to a tag used by
  `wordnet.synsets`. You will need to use this function in `doc_to_synsets`.
* **`document_path_similarity`**: computes the symmetrical path similarity
  between two documents by finding the synsets in each document using
  `doc_to_synsets`, then computing similarities using `similarity_score`.

You will need to finish writing the following functions:
* **`doc_to_synsets`**: returns a list of synsets in document. This function
  should first tokenize and part of speech tag the document using
  `nltk.word_tokenize` and `nltk.pos_tag`. Then it should find each token's
  corresponding synset using `wn.synsets(token, wordnet_tag)`. The first synset
  match should be used. If there is no match, that token is skipped.
* **`similarity_score`**: returns the normalized similarity score of a list of
  synsets (s1) onto a second list of synsets (s2). For each synset in s1, find
  the synset in s2 with the largest similarity value. Sum all of the largest
  similarity values together and normalize this value by dividing it by the
  number of largest similarity values found. Be careful with data types, which
  should be floats. Missing values should be ignored.

Once `doc_to_synsets` and `similarity_score` have been completed, submit to the
autograder which will run `test_document_path_similarity` to test that these
functions are running correctly.

*Do not modify the functions `convert_tag`, `document_path_similarity`, and
`test_document_path_similarity`.*


```{python}
import gensim
import nltk
import pandas as pd
import pickle
from nltk.corpus import wordnet as wn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score


def convert_tag(tag):
    """Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets."""

    tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}
    try:
        return tag_dict[tag[0]]
    except KeyError:
        return None


def doc_to_synsets(doc):
    """Return a list of Synsets in the document.

    Arguments:
        doc: String corresponding to the document.

    Returns:
        synsets: List of Synsets in the document.
    """

    # Tokenize the words in the document:
    toks = nltk.word_tokenize(doc)
    # assert type(toks) == list

    # Tag the words in the document:
    tags = nltk.pos_tag(toks)
    # assert type(tags) == list
    # assert type(tags[0]) == tuple

    # Find the first Synset for each word/tag combination.
    # If a Synset is not found for that combination, it is skipped.
    synsets = []
    for tok, tag in tags:
        tmp_synsets = wn.synsets(tok, pos=convert_tag(tag))
        # assert type(tmp_synsets) == list
        if len(tmp_synsets) > 0:
            synsets.append(tmp_synsets[0])

    # assert type(synsets[0]) == nltk.corpus.reader.wordnet.Synset

    return synsets


assert doc_to_synsets('Fish are nvqjp friends.') == [
    wn.synset('fish.n.01'),
    wn.synset('be.v.01'),
    wn.synset('friend.n.01')
]


def similarity_score(s1, s2):
    """Calculate the normalized similarity score of s1 onto s2.

    Arguments:
        s1: List of Synsets from doc_to_synsets.
        s2: List of Synsets from doc_to_synsets.

    Returns:
        score: Normalized similarity score of s1 onto s2.
    """

    # For each Synset in s1, find the Synset in s2 with the largest similarity value, and record this value:
    largest_similarities = []
    for synset_1 in s1:
        similarities = [synset_1.path_similarity(synset_2) for synset_2 in s2]
        try:
            largest_similarity = max([similarity for similarity in similarities if type(similarity) == float])
            largest_similarities.append(largest_similarity)
        except ValueError:
            continue

    # Compute the normalized similarity score of s1 onto s2.
    # Sum all of the largest similarity values.
    # Normalize this sum by dividing it by the number of largest similarity values found.
    score = sum(largest_similarities) / len(largest_similarities)

    return score


assert similarity_score(doc_to_synsets('I like cats'), doc_to_synsets('I like dogs')) == 0.73333333333333339


def document_path_similarity(doc1, doc2):
    """Compute the symmetrical similarity score for two documents.

    Arguments:
        doc1: String corresponding to the 1st document.
        doc2: String corresponding to the 2nd document.

    Returns:
        symm_score: Symmetrical similarity score corresponding to doc1 and doc2.
    """

    # Convert the documents to lists of Synsets:
    synsets1 = doc_to_synsets(doc1)
    synsets2 = doc_to_synsets(doc2)

    # Compute the similarity scores:
    score12 = similarity_score(synsets1, synsets2)
    score21 = similarity_score(synsets2, synsets1)

    # Compute the symmetrical similarity score:
    symm_score = (score12 + score21) / 2

    return symm_score
```


### test_document_path_similarity

Use this function to check if doc_to_synsets and similarity_score are correct.

*This function should return the similarity score as a float.*


```{python}
def test_document_path_similarity():
    doc1 = 'This is a function to test document_path_similarity.'
    doc2 = 'Use this function to see if your code in doc_to_synsets and similarity_score is correct!'
    return document_path_similarity(doc1, doc2)
```


`paraphrases` is a DataFrame which contains the following columns: `Quality`, `D1`, and `D2`.

`Quality` is an indicator variable which indicates if the two documents `D1`
and `D2` are paraphrases of one another (1 for paraphrase, 0 for not
paraphrase).


```{python}
# Use this DataFrame for questions most_similar_docs and label_accuracy:
paraphrases = pd.read_csv('paraphrases.csv')
paraphrases.head()
```


### most_similar_docs

Using `document_path_similarity`, find the pair of documents in `paraphrases`
which has the maximum similarity score.

*This function should return a tuple `(D1, D2, similarity_score)`.*


```{python}
def most_similar_docs():
    """Find the pair of documents in 'paraphrases' which has the maximum similarity score."""

    # Add to 'paraphrases' a column containing the similarity scores:
    paraphrases['Score'] = paraphrases.apply(lambda x: document_path_similarity(x.D1, x.D2), axis=1)

    # Find the index corresponding to the maximum similarity score:
    idx = paraphrases['Score'].idxmax()

    # Get the desired information:
    D1 = paraphrases.iloc[idx, 1]
    D2 = paraphrases.iloc[idx, 2]
    similarity_score = paraphrases.iloc[idx, 3]

    return (D1, D2, similarity_score)
```


### label_accuracy

Provide labels for the twenty pairs of documents by computing the similarity
for each pair using `document_path_similarity`. Let the classifier rule be that
if the score is greater than 0.75, label is paraphrase (1), else label is not
paraphrase (0). Report accuracy of the classifier using scikit-learn's
`accuracy_score`.

*This function should return a float.*


```{python}
def label_accuracy():
    """Compute the accuracy of the predictions made by using the similarity scores."""

    # Add to 'paraphrases' a column containing the similarity scores:
    paraphrases['Score'] = paraphrases.apply(lambda x: document_path_similarity(x.D1, x.D2), axis=1)

    # Use these results to predict the labels.
    # Add to 'paraphrases' a column containing the predicted labels.
    paraphrases['Prediction'] = paraphrases['Score'].apply(lambda x: 1 if x > 0.75 else 0)

    # Compute the accuracy of the classifier:
    accuracy = accuracy_score(paraphrases['Quality'], paraphrases['Prediction'])

    return accuracy
```


## Part 2 - Topic Modelling

For the second part of this assignment, you will use Gensim's LDA (Latent
Dirichlet Allocation) model to model topics in `newsgroup_data`. You will first
need to finish the code in the cell below by using
gensim.models.ldamodel.LdaModel constructor to estimate LDA model parameters on
the corpus, and save to the variable `ldamodel`. Extract 10 topics using
`corpus` and `id_map`, and with `passes=25` and `random_state=34`.


```{python}
# Load the list of documents:
with open('newsgroups', 'rb') as f:
    newsgroup_data = pickle.load(f)

# Use CountVectorizer to do the following:
# find three-letter tokens,
# remove stop words,
# remove tokens that don't appear in at least 20 documents,
# remove tokens that appear in more than 20% of the documents.
vect = CountVectorizer(
    min_df=20,
    max_df=0.2,
    stop_words='english',
    token_pattern='(?u)\\b\\w\\w\\w+\\b'
)

# Fit and transform:
X = vect.fit_transform(newsgroup_data)

# Convert sparse matrix to gensim corpus:
corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)

# Mapping from word IDs to words (to be used in LdaModel's id2word parameter):
id_map = dict((v, k) for k, v in vect.vocabulary_.items())
```


```{python}
# Use the gensim.models.ldamodel.LdaModel constructor to estimate LDA model
# parameters on the corpus:
ldamodel = gensim.models.ldamodel.LdaModel(
    corpus=corpus,
    num_topics=10,
    id2word=id_map,
    passes=25,
    random_state=34
)
```


### lda_topics

Using `ldamodel`, find a list of the 10 topics and the most significant 10
words in each topic. This should be structured as a list of 10 tuples where
each tuple takes on the form:

`(9, '0.068*"space" + 0.036*"nasa" + 0.021*"science" + 0.020*"edu" + 0.019*"data" + 0.017*"shuttle" + 0.015*"launch" + 0.015*"available" + 0.014*"center" + 0.014*"sci"')`

for example.

*This function should return a list of tuples.*


```{python}
def lda_topics():
    """Find a list of the 10 topics and the most significant 10 words in each topic."""
    return ldamodel.show_topics(num_topics=10, num_words=10, formatted=True)
```


### topic_distribution

For the new document `new_doc`, find the topic distribution. Remember to use
`vect.transform` on `new_doc`, and `Sparse2Corpus` to convert the sparse matrix
to a `gensim` corpus.

*This function should return a list of tuples, where each tuple is `(# topic, probability)`.*


```{python}
new_doc = ["\n\nIt's my understanding that the freezing will start to occur because \
of the\ngrowing distance of Pluto and Charon from the Sun, due to it's\nelliptical orbit. \
It is not due to shadowing effects. \n\n\nPluto can shadow Charon, and vice-versa.\n\nGeorge \
Krumins\n-- "]
```


```{python}
def topic_distribution():

    # Transform the new document:
    X_new = vect.transform(new_doc)

    # Convert sparse matrix to gensim corpus:
    corpus_new = gensim.matutils.Sparse2Corpus(X_new, documents_columns=False)

    # Get the topic distribution for the new document:
    distribution = list(ldamodel.get_document_topics(corpus_new))[0]

    return distribution
```


### topic_names

From the list of the following given topics, assign topic names to the topics
you found. If none of these names best matches the topics you found, create
a new 1-3 word "title" for the topic.

Topics: Health, Science, Automobiles, Politics, Government, Travel, Computers
& IT, Sports, Business, Society & Lifestyle, Religion, Education.

*This function should return a list of 10 strings.*


```{python}
def topic_names():

    names = [
        "Education",
        "Automobiles",
        "Computers & IT",
        "Religion",
        "Automobiles",
        "Sports",
        "Health",
        "Religion",
        "Computers & IT",
        "Science"
    ]

    return names
```
